Understood. I will conduct an in-depth research to develop a comprehensive solution document for your document pre-processing pipeline. This will include a structured technical solution design, covering table extraction, structured data storage, technology stack integration, scalability, and performance optimizations tailored to your listed industry use cases.

I will also ensure the document outlines workflows, implementation guidelines, and best practices while incorporating the required and optional technologies, including OpenShift, OpenShift AI, IBM Docling, Kubeflow, ElasticSearch, Neo4j, and IBM Granite Models.

I will notify you once the document is ready for your review.

# Overview of the Pre-Processing Workflow

**Architecture & Workflow:** The document ingestion pipeline is composed of modular steps running on OpenShift’s container platform. An incoming document (PDF, Word, HTML, image, etc.) is first captured (e.g. uploaded to a watch folder or via an API endpoint) and stored in a persistent volume or object storage (backed by OpenShift Data Foundation). This triggers a pipeline (using OpenShift Data Science Pipelines/Kubeflow or Tekton) which orchestrates the processing. The pipeline assigns a unique document ID and metadata, then classifies the document type/format. Next, the document is parsed and converted into a machine-readable structured form (JSON/Markdown) using IBM Docling, ensuring that layout elements like tables, text blocks, and images are identified ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Docling%20is%20built%20on%20top,reported%20when%20it%20was%20released)) ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=This%20is%20not%20about%20a,where%20it%20is%20being%20extracted)). Finally, the extracted structured data and raw text are stored in appropriate data stores and indexed for search and retrieval. Throughout, the pipeline logs each step and links results via the document ID for traceability (an audit trail) ([Building a Scalable Document Pre-Processing Pipeline | AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/building-a-scalable-document-pre-processing-pipeline/#:~:text=Amazon%20Simple%20Queue%20Service%20,passed%20along%20throughout%20the%20pipeline)) ([Building a Scalable Document Pre-Processing Pipeline | AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/building-a-scalable-document-pre-processing-pipeline/#:~:text=Aurora%20%20database,passed%20along%20throughout%20the%20pipeline)).

**Roles of Key Components:** *OpenShift* provides the scalable Kubernetes environment to deploy all services (e.g. Docling, OCR workers, database, pipeline controller) and manage containerized workloads. *OpenShift AI/OpenShift Data Science* adds specialized tooling for AI/ML – this includes a pipeline UI, Jupyter notebooks for development, and optimized scheduling for AI tasks (e.g. GPU support). *OpenShift Pipelines* (Tekton) or *Kubeflow Pipelines* handle the workflow orchestration: each step (document acquisition, parsing, post-processing, storage) is a containerized task in the pipeline DAG, with dependencies and parallelism as defined. For example, Tekton tasks could run OCR in parallel on different pages, or Kubeflow pipeline components could branch to handle text vs table processing concurrently. *IBM Docling* is the core parsing engine: it ingests the document in its original format and outputs structured JSON or Markdown representations, preserving the original layout and context ([Docling](https://simonwillison.net/2024/Nov/3/docling/#:~:text=uvx%20docling%20mydoc.pdf%20,to%20md)) ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,LangChain%2C%20LlamaIndex%2C%20Crew%20AI)). *Kubeflow* may also be used to manage any machine learning models involved (training fine-tuned OCR models or deploying AI models for extraction) and to serve those models within the pipeline. The entire process differentiates between **structured** and **unstructured** content – structured elements like tables are captured in a structured format (e.g. rows/columns in JSON), whereas unstructured text (free-form paragraphs) is retained as continuous text. This differentiation allows downstream components to handle each appropriately (for example, structured data can be loaded into a database, while unstructured text can be fed into an NLP model or vector index).

**Structured vs Unstructured Handling:** The pipeline treats document elements according to their nature. If the document is predominantly unstructured (e.g. narrative text), Docling ensures the reading order and textual context are preserved, producing a continuous text in the output JSON/Markdown ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,LangChain%2C%20LlamaIndex%2C%20Crew%20AI)). If the document contains **structured data** (tables, forms, lists), those segments are detected and extracted as discrete structured objects. For instance, a table spanning multiple pages will be recognized and combined into one logical table object ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=This%20is%20not%20about%20a,where%20it%20is%20being%20extracted)) rather than split by pages. This context-aware extraction is crucial – as one Red Hat/IBM team noted, *“if a table spans multiple pages, it must be extracted as a single table”* ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=This%20is%20not%20about%20a,where%20it%20is%20being%20extracted)). The pipeline uses element-aware techniques to maintain each element’s context and relationships. Unstructured sections and structured sections can thus be processed in parallel branches of the workflow if needed: one branch might transform raw text for indexing, while another branch formats table data for database storage. In summary, the workflow ingests diverse documents and, using a combination of OpenShift’s orchestration and IBM’s document parsing AI, transforms them into structured, analysis-ready data while preserving the original context (see **Figure 1** for a high-level architecture of such a pipeline).

 ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/)) *Figure 1: Example of a document ingestion pipeline architecture. The ingestion pipeline parses documents into structured JSON (separating text, tables, images) and then applies specialized extraction (OCR or vision models for tables/charts) before post-processing and storage. This pipeline runs on a Kubernetes/OpenShift platform, enabling parallel processing and integration with retrieval systems (e.g. vector databases for search).* ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=First%2C%20parse%20the%20PDFs%20to,is%20rendered%20as%20an%20image)) ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=%2A%20nv,reading%20order%20of%20the%20table))

# Table Data Extraction Strategies

**Multi-Format Table Extraction:** Extracting tables accurately requires different strategies depending on the document format and quality. For **digital PDFs and Word documents (DOCX)**, the table structure is often embedded in the file’s content stream or markup. Here, parsing tools can leverage the inherent structure: for example, PDF parsing libraries (PDFMiner, PDFPlumber) can retrieve text positions and detect tables by consistent column x-coordinates or drawing lines, and WordprocessingML (DOCX XML) explicitly denotes table rows and cells which can be read directly. IBM Docling uses advanced PDF understanding to interpret layout and identify tables in PDFs (including reading order and cell boundaries) ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,LangChain%2C%20LlamaIndex%2C%20Crew%20AI)). Traditional libraries like Camelot or Tabula can also be applied to PDFs: Camelot provides a high-level API to detect tables on each page and export them to formats like CSV or JSON ([Camelot: PDF Table Extraction for Humans — Camelot 1.0.0 documentation](https://camelot-py.readthedocs.io/#:~:text=,tables%5B0%5D.parsing_report)). In just a few lines, Camelot can read a PDF and output tables as pandas DataFrames or CSV files ([Camelot: PDF Table Extraction for Humans — Camelot 1.0.0 documentation](https://camelot-py.readthedocs.io/#:~:text=,tables%5B0%5D.parsing_report)). For example, `camelot.read_pdf('file.pdf')` will automatically detect tables on the PDF page and allow exporting them (Camelot supports multiple output formats and even gives an accuracy report for the extraction ([Camelot: PDF Table Extraction for Humans — Camelot 1.0.0 documentation](https://camelot-py.readthedocs.io/#:~:text=%3E%3E%3E%20tables.export%28%27foo.csv%27%2C%20f%3D%27csv%27%2C%20compress%3DTrue%29%20,24%2C%20%27order%27%3A%201))). These rule-based parsers work well when the PDF text is extractable and the tables have clear borders or column alignment. DOCX files can be handled by libraries (e.g. python-docx) that iterate over `document.tables` to retrieve rows and cell text, since the structure is explicitly defined in the file.

**Scanned PDFs and Images:** For documents that are scanned images (including PDFs that are essentially images of text), an OCR-based strategy is needed. A naive approach is to run OCR on the entire page and then attempt to decipher table structure from the plain text – but this often loses the cell boundaries and can be error-prone. A more robust strategy is *AI-powered layout analysis*: using computer vision models to detect table regions and cell structure within the image prior to OCR. IBM’s approach (as used in Docling) is to avoid full-page OCR whenever possible in favor of such vision models ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Traditionally%2C%20developers%20have%20relied%20on,visual%20elements%20on%20a%20page)). One model identifies and categorizes visual elements on the page – distinguishing paragraphs, images, and tables by analyzing layout geometry ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Docling%20is%20built%20on%20top,reported%20when%20it%20was%20released)). Another model called **TableFormer** then specifically handles table content: it transforms image-based tables into a machine-readable table format, effectively recognizing the grid of rows and columns from the image ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=The%20second%20model%2C%20TableFormer%2C%20is,recognition%20tools)). This two-step deep learning approach (layout analysis + table structure recognition) allows highly accurate extraction of tables from scans; in fact, IBM reported that TableFormer “outperformed leading table-recognition tools” in internal tests ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=The%20second%20model%2C%20TableFormer%2C%20is,recognition%20tools)). Concretely, the pipeline would first apply a table-detection model (similar to object detection) on each page image to localize tables. Once a table region is found, it is cropped and an OCR engine (like Tesseract or PaddleOCR) is run on each cell or using the guidance of the structure model to read text in the correct cell order ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=%2A%20nv,reading%20order%20of%20the%20table)) ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=,reading%20order%20of%20the%20table)). This yields the textual content of each cell along with the structure (which cell belongs to which row/column). By combining computer vision with OCR in this targeted way, the pipeline preserves the table formatting. Open-source alternatives for such AI-based table extraction include Microsoft’s TableNet/DeepTables or academic models trained on datasets like PubTabNet; however, IBM’s Docling has these capabilities integrated, leveraging the DocLayNet layout model and TableFormer under the hood ([Docling](https://simonwillison.net/2024/Nov/3/docling/#:~:text=Here%27s%20the%20Docling%20Technical%20Report,extracting%20structured%20data%20from%20tables)).

**HTML and Web-based Tables:** HTML documents usually contain tables as explicit `<table>` elements with `<tr>` (rows) and `<td>` (cells) tags. Extracting tables from HTML is straightforward using HTML parsers (BeautifulSoup, lxml): one can traverse the DOM to find table tags and then iterate through rows and cells. Since the structure is already given by the markup, the challenge is minimal – the key is to preserve any multi-level header or merged cells (colspan/rowspan) by capturing those attributes. The pipeline can convert HTML tables directly into JSON or DataFrame representations by reading the DOM. This is largely a rules-based parsing task (no ML needed), though if the HTML is generated dynamically or contains embedded images of tables, additional steps (rendering with a headless browser or OCR for images) may be necessary. 

**AI/NLP Techniques:** Beyond vision-based models, NLP techniques can assist in table extraction and understanding. For example, if table text has been extracted but needs cleaning or structuring, an NLP model or heuristic can identify column headers vs data rows by language patterns (headers often use singular nouns or abbreviations, data rows might be numeric etc.). Large language models (like an IBM Granite text model) could even be prompted to convert a chunk of text into a structured table given context – e.g., if OCR yields a chunk of text that visually was a table, an LLM could reconstruct it in CSV form by recognizing the patterns (though this would be a slower, AI-driven approach for edge cases). In general, deep learning models specialized for documents (such as LayoutLM for understanding document layout or graph-based models for table structure) can improve accuracy when rules fail. For instance, an NLP model could detect that a list of values is aligned in columns by analyzing spacing or delimiter characters, or it could use language context to guess the separation of cells. These techniques are complementary to explicit parsing: the pipeline might use a rules-based method as primary and fall back to an AI method if the table is complex or if the confidence score from the parser is low.

**Rules-based & Heuristics:** In many cases, simpler heuristic methods are effective, especially for well-formatted documents. Rules-based detection might involve looking for lines or borders in PDF (vector graphics that form cell borders) – Camelot’s **lattice** mode, for example, uses the presence of drawn lines to delineate table cells. Alternatively, a **stream** mode looks at the whitespace and alignment: text that is arranged in columns with consistent spacing can be grouped into a table structure by analyzing X/Y coordinates ([Python Libraries to Extract Tables From PDF: A Comparison](https://unstract.com/blog/extract-tables-from-pdf-python/#:~:text=Camelot)). These heuristics can be encoded without ML: e.g., “if five numbers appear at the same y-coordinate across the page, treat them as a row.” For plain text sources (like an OCR’d text file or a simple TXT report), one might use delimiters (commas, tabs, vertical bars) or consistent spacing to split columns. The pipeline could include a step where if a document is detected to be plain text with fixed-width formatting (common in older systems outputs), a custom parser uses positional slicing to break columns. Regular expressions can help identify patterns such as repeated sequences (for example, lines starting with dates or item numbers could indicate table rows). While these heuristic methods are less general than AI models, they are computationally cheap and can be very accurate for documents that follow a known template or structure.

In practice, a **hybrid approach** is best: the pipeline can attempt to use robust ML models (like Docling’s layout analysis) to identify tables and then apply domain-specific rules to fine-tune the parsing. For instance, after initial extraction, a post-processing script might verify the table integrity (e.g. every row has the same number of columns, numeric columns only contain digits, etc.) and correct or flag anomalies. The strategy also varies by document type: scanned historical records may rely more on OCR and image analysis, while modern digital documents exploit embedded structure. By incorporating multiple extraction techniques, the system ensures that **table data is captured with high fidelity** across varied input formats, fulfilling the goal of structured data preservation.

# Data Structuring and Storage Approaches

**Storing Extracted Table Data:** Once tables are extracted from documents, choosing the right storage format is key to downstream usability. A common approach is to use **JSON** to represent each table, since JSON can naturally capture hierarchical structure (table -> rows -> cells) and varying schema. For example, a table can be stored as a list of rows, where each row is an array of cell values or a key-value mapping of column name to value. JSON is flexible enough to handle tables with merged cells or complex nesting (you can represent nested tables as a cell containing another array). Docling’s output already provides a *“lossless JSON”* option ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,Simple%20and%20convenient%20CLI)), meaning the JSON preserves all content and formatting of the original document (including tables and their position). These JSON outputs can be stored as documents in a NoSQL database (like MongoDB or CouchDB) or simply in an object store (like Ceph/ODF or cloud storage) indexed by document ID. Storing as JSON ensures that no information is lost and that the structure can be reconstructed or traversed easily by applications. 

For certain use cases, **CSV or relational storage** might be preferred. If the extracted tables correspond to a known structured schema (e.g., a financial report where you know columns are Year, Revenue, Profit, etc.), converting them to CSV or directly inserting into a relational database table is useful for analytics and SQL querying. The pipeline could, for each extracted table, output a CSV file and store it in a data lake or load it into a SQL database. However, in a general multi-document pipeline, table schemas will differ between documents, so a relational database would either need a generic schema (like a key-value table or a table-of-tables design) or separate tables per document type, which can be complex to manage. Often a hybrid approach is used: store the raw table in JSON (for fidelity), and also produce a normalized view for specific tables of interest into relational form for easy querying. Another approach is to store tables in **pandas DataFrame** form during processing and then serialize to Parquet or Arrow for efficient columnar storage if performing data science tasks.

**Preserving Relationships Between Structured and Unstructured Data:** A crucial aspect of storage is maintaining links between the structured data (tables) and the original document context (unstructured parts). One method is to include metadata in the stored output. For example, each table JSON could carry fields like `document_id`, `page_number`, `section_title`, or even an anchor text that preceded the table (like “Table 1: Annual Results”). This metadata acts as a linkage so that, given a table, one can trace back to where it came from and what text surrounds it. Conversely, in the full document JSON, the table might be represented by a placeholder or object reference in the flow of text, preserving the insertion point. By **metadata tagging and document linkage**, the pipeline ensures that structured and unstructured pieces are not orphaned from each other. A query system can use these links to, for instance, retrieve the paragraphs that explain a table’s numbers. If multiple documents are ingested, a unique document ID combined with a table index (or table caption) can serve as a compound key to reference a specific table globally.

For more complex data relationships, using a **graph database (knowledge graph)** is recommended. In a graph model, each document can be a node, each table can be a node (or a sub-node linked to its document), and edges can represent relationships like *“contains”* or *“references.”* For instance, an edge from Document node to Table node denotes that the document contains that table. Further, if the content of a table has entities (say a “Company A” mentioned in the table and also in text elsewhere), those can be additional nodes connected via edges (e.g., Table -> Company node, Paragraph -> Company node). Neo4j or other graph databases excel at storing such networks of information, allowing queries like “find all tables that relate to Company A in regulatory reports”. The graph structure explicitly preserves relationships that are implicit in documents. It’s flexible: new nodes/edges can be added as more links are discovered ([Vector database vs. graph database: Understanding the differences | Elastic Blog](https://www.elastic.co/blog/vector-database-vs-graph-database#:~:text=match%20at%20L421%20graph%20databases,databases%20ideal%20for%20many%20applications)) ([Vector database vs. graph database: Understanding the differences | Elastic Blog](https://www.elastic.co/blog/vector-database-vs-graph-database#:~:text=,within%20a%20single%20interconnected%20graph)). This is especially powerful for cross-document analysis in domains like law enforcement (connecting pieces of evidence across reports) or research (linking citations and data across papers).

**Vector and Metadata Storage for Search:** To support semantic search or retrieval-augmented QA, vector databases (or vector indexes) are used to store embeddings of text and even table data. After extraction, the pipeline can generate embeddings (using an encoder model) for each meaningful chunk – e.g. each paragraph of text and each table (perhaps by converting the table to a string or embedding its cells separately). These embeddings, along with metadata (document ID, section, etc.), are stored in a **vector database** or an index like OpenSearch/Elasticsearch with KNN enabled. A vector database represents data as high-dimensional numeric vectors, placing similar content close together in that space ([Vector database vs. graph database: Understanding the differences | Elastic Blog](https://www.elastic.co/blog/vector-database-vs-graph-database#:~:text=Instead%20of%20rows%20and%20columns%2C,from%20planets%20with%20fewer%20similarities)). This allows semantic similarity searches: a query embedding can retrieve the most similar table or text embedding, enabling questions like “find the table describing 2024 revenue” even if the wording differs. We can use Elasticsearch (with its vector search capabilities) as the vector store – it can hold both traditional inverted index for keywords and vector index for semantic similarity, giving a dual advantage. When storing in such an index, we include metadata fields (document ID, page, etc.) so that search results can be contextualized and linked back to the source.

**When to use Vector vs Graph vs Relational:** Each storage has its strengths. A **vector store** is ideal for discovery and unstructured query – fast similarity matching but it doesn’t inherently store relationships between data points ([Vector database vs. graph database: Knowledge Graph impact - Writer](https://writer.com/engineering/vector-database-vs-graph-database/#:~:text=Writer%20writer,for%20use%20in%20business%20settings)). A **graph store** explicitly stores relationships and is ideal when queries involve traversing connections (e.g. finding networks of entities, or tracing a chain of references) ([Vector database vs. graph database: Understanding the differences | Elastic Blog](https://www.elastic.co/blog/vector-database-vs-graph-database#:~:text=What%20are%20graph%20databases%3F)). If the use case involves a lot of *content search* (e.g. “find documents discussing X” or “retrieve the section about Y”), a vector DB or search index will be heavily used. If the use case involves *analytic queries on structured data* (e.g. “what was the total in this table?” or aggregating values across tables), then having the data in a structured database (relational or at least queryable CSV/Parquet) is useful. In many solutions, a combination is used: the pipeline could store the raw outputs in JSON (as the source of truth), index the text in Elasticsearch for search, and also populate a Neo4j graph with key nodes/edges to capture relationships that matter (like linking related documents, or linking tables with similar schema across years, etc.). This multi-faceted storage ensures that whether one needs to do a full-text search, a semantic similarity query, or a relational query, the data is available in the optimal form. 

**Metadata Best Practices:** Regardless of storage type, robust metadata is essential. Each data element (document, page, table, image, paragraph) should have tags such as source, creation date, document type, language, and any domain-specific tags (e.g., for healthcare, maybe patient vs provider document, for finance maybe report type or quarter). Metadata helps enforce compliance too – e.g. tagging a document with sensitivity level (public vs confidential) can inform how it’s handled in downstream systems. The pipeline should capture whatever metadata is available (file name, headers, classification markings, etc.) and propagate those tags through to the storage. This way, structured table data is not just floating data in a database; it’s anchored to provenance information and context, enabling trustworthy and traceable AI use cases.

# Technology Recommendations

**IBM Docling for Parsing & Context Preservation:** IBM Docling is a centerpiece of this solution, as it provides an out-of-the-box toolkit for robust document parsing. By integrating Docling into the pipeline, we leverage its ability to handle diverse formats (PDF, DOCX, PPTX, images, HTML, etc.) and output structured representations ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=Docling%20is%20an%20upstream%20open,answering%20tasks)) ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,LangChain%2C%20LlamaIndex%2C%20Crew%20AI)). Docling’s advantage is that it uses AI models to preserve document structure: it will detect tables, lists, headings, even multi-column layouts, and convert them into a unified JSON (or Markdown) while *maintaining context*. This means even after parsing, we know exactly where each piece of data came from (page, section) and in what order. Docling’s JSON output is essentially a *DoclingDocument* object tree that contains all elements (paragraphs, tables, figures) with their content and metadata ([Docling](https://simonwillison.net/2024/Nov/3/docling/#:~:text=uvx%20docling%20mydoc.pdf%20,to%20md)). For our table extraction focus, Docling ensures that tables are identified as distinct elements and not mixed with running text. It even can output tables directly as pandas DataFrames via its Python API ([Docling](https://simonwillison.net/2024/Nov/3/docling/#:~:text=from%20docling,export_to_dataframe%28%29%20print%28df)), which is convenient for immediate data manipulation. We recommend using Docling as the primary parser in the pipeline because it yields high accuracy (thanks to IBM’s DocLayNet and TableFormer models) and it’s extensible/open-source. Additionally, Docling supports **extensive OCR for scanned documents** and can smartly avoid OCR when not needed ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Traditionally%2C%20developers%20have%20relied%20on,visual%20elements%20on%20a%20page)), which improves both speed and accuracy. Another benefit is that Docling can run locally within our OpenShift cluster (no external API calls), aligning with data privacy requirements (e.g., processing sensitive healthcare documents within a HIPAA-compliant environment, or processing law enforcement data in an air-gapped network) ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,Simple%20and%20convenient%20CLI)). In summary, IBM Docling should be used as the document ingestion engine to parse and convert incoming files into structured, context-rich data, ensuring unstructured text and structured tables are retained faithfully.

**OpenShift AI, Kubeflow, and Pipelines:** OpenShift AI (which encompasses Red Hat OpenShift Data Science and related services) provides the infrastructure to operationalize this workflow. We recommend using **OpenShift Data Science Pipelines** (Kubeflow Pipelines v2 on OpenShift) to orchestrate the multi-step process. Kubeflow Pipelines allow us to define the workflow as a series of components (containerized tasks) with inputs/outputs, and the pipeline can be versioned and reused. For example, one component could be “Parse Document with Docling,” another “Extract Tables to JSON/CSV,” another “Index Data in ElasticSearch,” etc. These pipelines can be triggered programmatically or via events. OpenShift’s integration means we can monitor and manage these pipelines through the OpenShift console. Kubeflow on OpenShift also gives us the ability to incorporate Jupyter Notebook steps or model training if needed (say we want to fine-tune an OCR model on new data, we could add a training step in the pipeline). In our context, Kubeflow Pipeline would primarily be used for orchestration of the data flow and any ML model inference tasks (for instance, if using a custom deep learning model for table detection outside of Docling, Kubeflow can orchestrate that inference on a GPU node). **OpenShift Pipelines (Tekton)** could alternatively be used, especially if the team prefers Tekton’s CI/CD style pipeline definitions; Tekton is well-suited for data ingestion tasks too and can run in parallel, with triggers. In fact, an architecture can combine them: Tekton for high-level orchestration and Kubeflow Pipelines for complex ML subflows, or vice-versa, but often one or the other is chosen for simplicity. The key recommendation is to utilize the pipeline features of OpenShift to automate the entire pre-processing workflow – this ensures repeatability and easy scaling (pipelines can run concurrently for multiple documents). Moreover, pipeline systems keep metadata of runs, which is useful for auditing (you can track which pipeline run processed which document and what the outcomes were).

**Messaging and Integration:** We also advise integrating Red Hat AMQ Streams (Kafka on OpenShift) for decoupling ingestion. For instance, a drop of a file into Object Storage or a new document event can send a message to a Kafka topic. A pipeline trigger can listen to this topic and start the processing pipeline for each message. This decoupling via a message queue allows robust ingestion at scale – producers of documents and consumers (the pipeline) can scale independently. Kafka also enables buffering: if a surge of documents arrives, they queue up and pipelines pick them as resources allow, smoothing out spikes. OpenShift’s ecosystem makes it straightforward to connect AMQ Streams with Tekton triggers or Kubeflow (via event listeners).

**Elasticsearch (Vector DB) vs Neo4j (Graph DB):** We recommend using **Elasticsearch (or OpenSearch)** as the vector-capable search index for the extracted textual data. Elasticsearch can index the raw text and also store vector embeddings for semantic search, which is useful for RAG use cases. By using Elasticsearch, you can perform keyword queries, full-text searches, and approximate nearest neighbor searches in the same system. This is ideal for supporting a question-answering system or a user interface where analysts search through ingested documents. In contrast, **Neo4j** is recommended for storing and querying explicit relationships in the data. If your use cases involve tracing connections (e.g., linking a person’s name in one document to an address in another, or linking a contract document to related purchase orders in procurement), Neo4j provides graph query capabilities (Cypher queries to find paths, etc.) that a vector DB or relational DB cannot easily match ([Vector database vs. graph database: Knowledge Graph impact - Writer](https://writer.com/engineering/vector-database-vs-graph-database/#:~:text=searches%20but%20lack%20the%20context,for%20use%20in%20business%20settings)). In summary, use Elasticsearch when the task is *information retrieval* (finding relevant pieces of text or semantic similarity) and use Neo4j when the task is *knowledge mining* (understanding relationships and networks in the data). These technologies can be used in tandem: for example, after extracting data, you might index all text in Elasticsearch for search, but also populate Neo4j with nodes for entities (like names, organizations) extracted from that text. The question of which to use also depends on scale and expertise: Elastic can handle large volumes of text and provides analytics on text, whereas Neo4j handles complex queries on interconnected data efficiently. Given the multi-industry requirement (healthcare, finance, etc.), it’s likely beneficial to have both: Elastic to quickly retrieve relevant documents or snippets (with compliance filters applied for GDPR/HIPAA), and Neo4j to allow analysts to explore relationships (for example, a fraud investigator could traverse a graph of transactions and documents).

**IBM Granite Models Integration:** IBM’s **Granite** foundation models are a set of large AI models (including large language models and multimodal models) that can greatly enhance the accuracy and capability of the pipeline. We recommend incorporating Granite models in two ways: (1) **for AI-driven extraction enhancements**, and (2) **for advanced post-processing/analysis**. On the extraction side, Granite includes state-of-the-art models for language and vision. For instance, the Granite series has large language models with up to 128k token context windows ([Document Question Answering with Granite | IBM](https://www.ibm.com/think/tutorials/build-document-question-answering-system-with-docling-and-granite#:~:text=When%20an%20LLM%20has%20a,is%20also%20available%20on%C2%A0Hugging%20Face)), which means a Granite LLM could ingest an entire lengthy document (or its extracted JSON) and answer questions or check consistency across the whole document. This is useful for verification – e.g., after extracting a table, an LLM could be asked to summarize it or to double-check if the sum of a column matches a value stated in the text, providing a form of QA. More directly, IBM’s document vision models (like the layout parser and TableFormer described earlier) are effectively part of the Granite ecosystem (IBM has indicated plans to integrate the extracted data into a *forthcoming IBM Granite multimodal model* ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=It%20was%20also%20used%20to,forthcoming%20IBM%20Granite%20multimodal%20model))). If available, a Granite multimodal model might take an image or PDF page as input and output structured data, offering an end-to-end deep learning approach to document understanding. As these become available, the pipeline could offload certain tasks to such models (for example, use a Granite vision transformer to handle extremely complex layouts or to extract information from charts/graphs in the documents, something Docling will soon support per its roadmap ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Researchers%20plan%20to%20build%20out,equations%2C%20charts%2C%20and%20business%20forms))). Additionally, Granite NLP models can be fine-tuned or prompted for domain-specific extraction – for instance, a Granite model could be prompted to extract all patient names from a medical form or all legal citations from a court document, complementing the generic parsing with domain intelligence.

Practically, incorporating Granite might involve deploying the model on OpenShift (using IBM’s watsonx.ai or similar if available on-prem) and calling it at certain pipeline steps. Since Granite models are resource-intensive, you’d use them selectively – perhaps to re-process content that wasn’t confidently parsed by simpler methods, or to provide an AI-powered validation layer. They also shine in *multilingual* contexts: Granite NLP models (if trained on multilingual data) can better handle extraction in languages where rule-based methods falter. For example, in a multilingual procurement scenario, an English-trained parser might struggle with an Arabic table, but a multilingual foundation model could parse or translate it more effectively. 

In summary, IBM Granite models can be seen as an “AI supercharger” for the pipeline: use their **vision capabilities** to improve OCR/layout detection beyond traditional tools, and use their **language capabilities** to ensure accuracy and extract deeper insights (like understanding the content of the table, not just the raw text). This will future-proof the pipeline, as more powerful foundation models can be plugged in to continuously improve extraction quality. Moreover, because Granite models (like Granite-3.1-8B) are designed for enterprise with large context ([Document Question Answering with Granite | IBM](https://www.ibm.com/think/tutorials/build-document-question-answering-system-with-docling-and-granite#:~:text=When%20an%20LLM%20has%20a,is%20also%20available%20on%C2%A0Hugging%20Face)), they align well with our goal of handling large documents and preserving all their information for AI use cases.

# Scalability & Performance Considerations

**Large-Scale Ingestion:** The system is designed to handle high volumes of documents and large document sizes without bottlenecks. OpenShift’s horizontal scaling is leveraged to run many pipeline instances in parallel. For example, if thousands of documents must be ingested per day, we can configure the pipeline controller to spawn multiple pods (workers) to process documents concurrently. Using a messaging queue (Kafka) as mentioned allows a fleet of consumer pods to pull tasks as they are free, enabling a **work-queue pattern** that naturally scales out. We recommend partitioning the workload by document or by document segments. At a coarse level, each document is processed independently in parallel (no shared state until results need merging, which is only per document). This embarrassingly parallel approach means if we need higher throughput, we simply add more compute nodes or pods. OpenShift can schedule these across the cluster, and auto-scaling (with Kubernetes Horizontal Pod Autoscalers or KEDA with Kafka event scaling) can be employed to dynamically adjust capacity based on queue length or CPU usage.

For **large documents** (hundreds of pages or very large PDF files), consider a strategy of internal chunking to improve performance. One approach is similar to what AWS suggests: split a document into individual pages or sections, process them concurrently, then aggregate the results ([Building a Scalable Document Pre-Processing Pipeline | AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/building-a-scalable-document-pre-processing-pipeline/#:~:text=In%20order%20to%20maximize%20grip,to%20sending%20it%20for%20inference)) ([Building a Scalable Document Pre-Processing Pipeline | AWS Architecture Blog](https://aws.amazon.com/blogs/architecture/building-a-scalable-document-pre-processing-pipeline/#:~:text=Each%20of%20these%20queues%20is,and%20ultimately%2C%20into%20individual%20pages)). Our pipeline could include a “Splitter” step that, if a document exceeds a certain size (say >N pages), splits it into smaller chunks (each chunk could be a range of pages). Each chunk can then be processed by the parsing engine (Docling) in parallel (for example, run one instance per 50 pages). This can reduce memory usage per pod and speed up processing by parallelizing work. **However**, splitting must be done carefully to not break context – if a table spans pages, splitting right in the middle of it could cause that table to be seen as two pieces. Mitigation for this is to use intelligent splitting: e.g., detect if page breaks occur inside tables and, if so, treat that set of pages as one chunk. Docling’s context-aware nature helps here (it can accept a range of pages and still merge a table across them). After parallel processing of chunks, a “Merger” step can combine the JSON outputs back into one document JSON. This adds complexity and might only be necessary for extremely large docs, so we weigh the trade-off. In many cases, Docling can handle a whole document in one go (especially with proper resource allocation, like enough RAM/CPU or even GPU acceleration for its models), so chunking is optional.

**Parallel Processing Techniques:** The pipeline should exploit parallelism at multiple levels: (1) *Document-level parallelism* (multiple docs at once, as discussed), and (2) *Task-level parallelism within a document*. Task-level parallelism might mean, for instance, extracting text and images in parallel. If using Tekton or Argo, one could define that after an initial file load, two tasks run: one does text extraction (perhaps for plain text parts) and another does table extraction (focusing on layouts). In practice, since Docling already extracts everything in one pass, we might not split text vs table extraction into separate tasks. But we could parallelize *independent subtasks* like OCR on multiple images in the doc simultaneously if they were extracted. Kubeflow pipelines (DAG) allow parallel branches – e.g., branch to handle tables and branch to handle figures at the same time, then join. Another angle is using distributed computing frameworks (like Apache Spark or Ray) if we needed to pre-process extremely large datasets of documents; those could be deployed on OpenShift as well. However, given our architecture with containerized tasks and message queues, we likely have enough parallelism.

**Resource Optimization:** Each step can be optimized for speed. For example, the Docling parsing step can be resource-intensive (loading AI models). We can optimize it by enabling model reuse: instead of spinning up a fresh Docling environment for each document (which would reload models each time), we could run Docling as a service (a long-lived container that keeps the models in memory and processes multiple docs sequentially). This could be implemented as a pool of Docling server pods and the pipeline sends requests to it (like an inference service). This avoids repetitive initialization overhead. OpenShift AI allows deploying such model-serving instances (e.g. using KServe or custom REST services). We also ensure that the OCR library (if Tesseract is used for some reason) is configured with the needed language packs ahead of time to avoid on-the-fly loading. Using GPUs can significantly speed up the vision models (the layout analysis and TableFormer). If available, we schedule the Docling task onto GPU nodes. Since Docling uses PyTorch under the covers for its models, a GPU can accelerate that by orders of magnitude, enabling near real-time processing even for complex pages.

**High-Throughput Pipeline Design:** To achieve high throughput, avoid any sequential bottleneck in the pipeline. For instance, writing to storage should be done asynchronously where possible. If a pipeline step writes the extracted JSON to a database, it should not block further processing of other documents. In Tekton, one might offload that to an event (e.g. publish an event that another service picks up to do DB insertion) if direct insertion is slow. Similarly, the use of streaming can help: as soon as one page is processed, results could start streaming to storage while the rest is processing. In practice, a simpler approach is to ensure each pipeline run is stateless and fast, and let many runs work in parallel. 

We also consider the network and I/O: large PDFs can be tens of MBs, so reading from storage and writing outputs should be done on efficient channels. Using OpenShift Data Foundation’s object storage (S3 interface) is recommended for large binaries – it’s optimized for throughput. The pipeline can fetch the file from ODF S3, process in memory, and then write results back. If using persistent volumes, ensure they are high-IOPS for quick read/write.

**Scalability Tests & Tuning:** We would perform load testing by simulating a bulk ingestion (say 1000 documents of mixed types) and measure pipeline execution times. Based on this, we tune the number of parallel workers. OpenShift’s monitoring (Prometheus metrics for CPU, mem, etc.) will guide if we need more replicas or if some step is lagging. We also set up **timeouts and retries** in the pipeline for robustness – e.g., if OCR on a page hangs or fails, have a retry or send that document to a error queue for later review, rather than stalling the pipeline. Tekton and Kubeflow support retries on failed steps.

**Large Context and Batch Processing:** In some use cases, we might accumulate documents and process in batch during off-peak hours (for example, a nightly job to ingest all new docs). The solution should handle both streaming (real-time) and batch modes. OpenShift can handle batch jobs by scaling up a lot of pods for the duration of the batch and scaling down after. The pipeline itself remains the same definition.

In terms of performance, IBM Docling already optimized a key part: by avoiding brute-force OCR, it improved speed dramatically (up to 30x faster by using layout models) ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Traditionally%2C%20developers%20have%20relied%20on,visual%20elements%20on%20a%20page)). This is a boon for scalability, because OCR is typically the slowest part. By leveraging Docling’s efficient methods, we reduce CPU cycles. We also note that Docling’s models (layout analysis) came close to human-level accuracy in identifying elements ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Next%2C%20it%20identifies%20and%20classifies,reported%20when%20it%20was%20released)), meaning fewer errors to correct (which can also slow pipelines if manual intervention is needed).

In conclusion, the pipeline scales horizontally on OpenShift and is optimized to run many documents in parallel. By using event-driven triggers, parallel tasks, and efficient model usage, we ensure high throughput. The design can comfortably handle multi-GB document sets and can be tuned up to even larger scales (the IBM/Red Hat team plans to process 1.8 **billion** PDFs with Docling for a future model ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=It%20was%20also%20used%20to,forthcoming%20IBM%20Granite%20multimodal%20model)) – while our deployment may not be that extreme, it gives confidence that the approach is inherently scalable). The architecture will also include monitoring and auto-scaling hooks so that performance can be maintained under varying loads.

# Challenges & Mitigation Strategies

**Complex, Nested, or Multi-page Tables:** One of the toughest challenges is correctly extracting tables that are not simple one-page grids. Multi-page tables (where a single logical table continues from one page to the next) could be split by less sophisticated parsers. Our strategy to mitigate this is using Docling’s context-aware extraction which recognizes page-spanning tables and merges them ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=This%20is%20not%20about%20a,where%20it%20is%20being%20extracted)). If not using Docling for some reason, we would implement a post-processing step: when a page break is encountered, check if the next page starts with a table header or similar structure that indicates a continuation (sometimes documents explicitly repeat the header row or say “(continued)” ). The pipeline can then concatenate those parts into one table. For nested tables (a table within a cell of a larger table), the JSON representation must preserve this hierarchy. Docling’s JSON can nest elements, so a table inside a cell will appear as a sub-table element. If using custom parsing, we ensure to detect if a cell content looks like a table (e.g., contains consistent delimiters or lines) and then treat it as such. While nested tables are relatively rare, forms sometimes have tables within tables for layout. Testing on such documents and verifying the JSON structure will be important. In cases of spanning cells (rowspan/colspan), we preserve those indicators – e.g., in JSON, a cell may have properties like “colspan: 2” if it spans two columns. This way, when rendering or analyzing, the structure isn’t misinterpreted. The challenge of nested structures is essentially maintaining a tree of content – our use of a graph/JSON model inherently handles that, as opposed to flattening it.

**Ensuring Data Consistency Across Formats:** Different input formats might yield slight differences in extracted data (for instance, a Word document’s table might include hidden cells or specific number formatting that a PDF version doesn’t). To ensure consistency, we apply normalization steps. One mitigation is to convert everything to a common intermediate format when possible – for example, if both Word and PDF versions exist, choose one format as the primary or convert one to the other (some pipelines convert Word to PDF then parse, or vice versa using PDF to text then compare with Word text). We also normalize number formats (removing thousands separators for storing, or standardizing date formats) so that data can be compared across documents. If the same data appears in multiple documents (like a multilingual version of a procurement form), we keep track of those linkages, possibly via the graph DB, to reconcile any differences. Consistency is also enforced by schema checking: if we expect certain columns to be numeric, and an extraction yields a non-numeric, we flag it for review or attempt a re-run with a different method. Additionally, we must ensure that when moving data into different storage, the fidelity remains. For example, JSON is great for preservation, but if we export to CSV, certain things like newline within a cell might break format – we mitigate that by quoting or replacing internal newlines. Using UTF-8 consistently and handling encoding issues is also important (to avoid losing special characters from OCR, especially in multilingual scenarios).

**OCR Errors in Scanned Docs:** OCR can introduce errors like misrecognized characters (“O” vs “0”, “l” vs “1”, etc.), which could corrupt table data. To mitigate this, we use high-quality OCR models (PaddleOCR or Tesseract with appropriate language packs) and consider fine-tuning them on our document samples if needed. Docling’s approach of using visual context helps reduce errors because it’s not solely relying on character recognition in isolation ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=Traditionally%2C%20developers%20have%20relied%20on,visual%20elements%20on%20a%20page)). Nevertheless, for critical fields, we implement validation rules. For instance, in a financial table, if a certain column is supposed to be numeric, we can detect if OCR produced a non-numeric string and try to correct it (maybe by looking at the nearest valid number or using an alternative OCR engine on that region). Another mitigation is voting or ensemble OCR: use two OCR engines and compare results for suspicious values. If they disagree significantly, flag for manual check or use the one with higher confidence. Spell-checking or dictionary-based correction is useful for text fields – e.g., OCR of a medical term could be corrected by referencing a medical dictionary. In multilingual contexts, setting the OCR engine to the correct language is crucial (Tesseract allows specifying language, which improves accuracy). If a document has multiple languages, we could run language detection on each page or segment and OCR each with the appropriate language model to reduce errors.

**Handling Different Table Structures:** Some tables can be very irregular (ragged rows, varying number of columns, or complex headers that span multiple rows). The challenge is representing these in a consistent way. Our mitigation is to use a flexible data model (like list of rows, where each row is list of cells, and allow some rows to have fewer cells if they logically do). We capture header hierarchy: e.g., if a header spans two columns, we might insert empty placeholder in the second column in the row below in JSON to indicate it spans. This way the number of columns per row can vary, but we know how they align. When storing in relational form, such tables are hard – we might flatten multi-level headers (concatenate them) or treat the table as two linked tables (one for the headers description, one for the data). These are case-by-case decisions. The key is no data or relation is lost: even if a table is too complex to fit a neat CSV, the JSON will hold its exact structure, and that JSON is always available for reference. 

**Multilingual and Locale Issues:** Since the pipeline targets multilingual documents, there are challenges in parsing different scripts (like Arabic or Chinese tables) and formatting (e.g., European number format uses comma as decimal separator). We address this by using OCR engines that support those scripts and by localizing parsing logic. For example, for Arabic, we ensure right-to-left text is handled (Docling and OCR need to be configured for bidi text). For locale, we might normalize numbers by replacing locale-specific punctuation after extraction (e.g., convert “1.234,56” to “1234.56” if it’s a number with European format). Also, language detection can route the document to specific processing pipelines if needed (maybe a special handling for double-byte characters, etc.). Testing on a variety of languages and scripts will help refine these settings.

**Performance vs Accuracy Trade-offs:** A challenge is balancing speed and accuracy. High accuracy might require using an expensive model (like an 8B parameter LLM) or doing multiple passes; but that can slow the pipeline. We mitigate this by a tiered approach: use fast methods first (layout model, simple OCR) which will handle the majority of cases quickly. Only if those fail or produce low confidence, invoke the heavier processes (like a Granite LLM to interpret a confusing table). This way, we don’t pay the cost for every document, only the tricky ones. We can measure confidence by metrics like OCR confidence scores or Docling’s internal model confidence. For example, if a table extraction yields an internal accuracy measure below a threshold, that triggers a second attempt with a different approach.

**Regulatory Compliance & Security:** In industries like healthcare (HIPAA) and finance (GDPR), a big challenge is ensuring that sensitive data is protected through the pipeline. This is not a data extraction challenge per se, but a design mandate: we must ensure all data stays within authorized bounds. Mitigations include encrypting data at rest (OpenShift Data Foundation can encrypt the storage, and any database should use encryption), and in transit (enable TLS on all internal APIs, such as calls to Elastic or Neo4j, and use OpenShift Service Mesh if needed for mTLS between services). Role-Based Access Control (RBAC) on OpenShift means only certain service accounts can run the pipeline and access the data stores. For example, the pipeline’s account might have write access to a database, and a separate read-only account is used by applications to query it, preventing unauthorized modification. We also should redact or tokenize sensitive fields if not needed in raw form. If storing documents for AI, maybe we remove social security numbers or patient IDs and store a hash instead, to comply with privacy laws, unless absolutely needed. Audit logging is important: every time a document is processed or accessed, log the event. This helps with compliance audits and incident response (knowing who accessed what). OpenShift’s monitoring and logging stack can be configured to capture these logs securely. Finally, if working with national security data, deployment in an air-gapped OpenShift cluster might be required – Docling’s local execution (no internet needed) supports that scenario ([Docling - Docling](https://ds4sd.github.io/docling/#:~:text=,Simple%20and%20convenient%20CLI)). All models and tools would be hosted internally (which is why open-source tools like Docling are helpful – no external API calls to parse documents).

By anticipating these challenges and implementing the above strategies, the pipeline becomes robust and reliable. There will always be edge cases (a bizarrely formatted table, a nearly illegible scan, etc.), but the system is designed to handle the common cases automatically and flag the truly hard cases for human review or iterative improvement. Continuous monitoring of errors (like counting how many docs end up in a “failure” queue due to parsing issues) will guide future enhancements, possibly adding new rules or training models on new examples to continuously improve the pre-processing accuracy.

# Example Pipeline Implementation

To illustrate the solution, consider an example implementation flow and some pseudocode for key steps:

**Pipeline Definition:** We define a pipeline (using Kubeflow Pipelines SDK or Tekton YAML) with the following steps:

1. **Ingestion Trigger** – Triggered by an event (e.g., new file in object storage or a Kafka message with document URI). This step records the document ID, source, and saves the file to a working directory (if not already in persistent storage accessible to the cluster).
2. **File Type Classification** – A lightweight step (could be as simple as checking file extension or using `libmagic`) that determines the type: e.g., `type = PDF (scanned)` or `PDF (digital)` or `Word` or `Image`. It can set pipeline parameters for later steps (like `needs_ocr=True/False`).
3. **Document Parsing (Docling)** – This step uses IBM Docling’s Python API. For example, a code snippet: 

   ```python
   from docling.document_converter import DocumentConverter
   converter = DocumentConverter(ocr_languages=["eng","spa","ara"])  # specify OCR languages needed
   result = converter.convert(input_file_path)
   doc_json = result.document.export_to_json()  # get structured JSON with tables, text, etc.
   with open('output.json', 'w') as f:
       f.write(doc_json)
   ```
   This will produce an `output.json` that contains the full parsed content. Docling will automatically perform OCR if needed (e.g., for scanned PDFs) and include extracted text in the JSON. It will use its internal table detection so that tables are represented in an organized way (as part of the JSON structure). If Docling encounters an issue (say, extremely complex layout), it will still output whatever it can parse. The pipeline can be set to examine `result.document.tables` – e.g., `len(result.document.tables)` to see how many tables were found, and log that for verification.

4. **Table Extraction & Conversion** – Although the JSON contains the tables, this step can further process tables if needed. For example, using Docling’s result, we can convert each table to a CSV or DataFrame. Docling makes this easy: `for table in result.document.tables: table_df = table.export_to_dataframe()`, then one could do `table_df.to_csv("table_{i}.csv")`. Alternatively, if using Camelot/Tabula for PDFs outside Docling, this step would run those tools. For instance, using Camelot on a PDF:
   
   ```python
   import camelot
   tables = camelot.read_pdf(input_file_path, pages='all')
   for i, table in enumerate(tables):
       table.to_csv(f"table_{i}.csv")
   ```
   
    ([Camelot: PDF Table Extraction for Humans — Camelot 1.0.0 documentation](https://camelot-py.readthedocs.io/#:~:text=,tables%5B0%5D.parsing_report)). This example would produce CSVs for each detected table. We might use such a method if we decided not to use Docling for table extraction, but given Docling covers it, consider this just an alternative. If the document was HTML or DOCX, we could instead use a small parser (BeautifulSoup or python-docx) in this step, but again, if Docling already parsed it, we have the content. This step might be more about formatting and cleaning the extracted table data (e.g., removing line breaks in cells, trimming whitespace, converting data types).

5. **Storage & Indexing** – In this step, the pipeline takes the outputs (the master JSON, plus any separate table files) and stores them. For example:
   - Upload the `output.json` to an object storage bucket or a database. This could be done with an API call or CLI, e.g., using `aws s3 cp output.json s3://mybucket/docs/json/` (if using an S3 interface) or using a Python client for a document database.
   - If using a relational database for some structured data, insert the table CSVs. For instance, using Python’s psycopg2 to insert into Postgres or JDBC for other DBs.
   - Index in Elasticsearch: one could use the Elasticsearch Python client to index the document. For instance:
     ```python
     import elasticsearch
     es = elasticsearch.Elasticsearch([{"host": ES_HOST}])
     # Index full text for search
     es.index(index="documents", id=document_id, body={"text": result.document.text, "metadata": {...}})
     # Index each table as separate entry for specific table search (if desired)
     for t_index, table in enumerate(result.document.tables):
         es.index(index="tables", id=f"{document_id}_{t_index}", 
                  body={"table_json": table.to_dict(), "document_id": document_id, "caption": table.caption, ...})
     ```
     In this snippet, we index the plain text of the document for full-text search, and also index each table (maybe storing it as JSON or as flattened text) in a separate index. If vector embeddings are to be stored, an earlier step would have computed embeddings (using, say, sentence-transformers or IBM Granite model) and those would be included in the index call as a dense vector field.
   - Update Neo4j: using Neo4j’s Bolt protocol via Python (e.g., `neo4j` driver), we can create nodes and relationships. For example:
     ```python
     from neo4j import GraphDatabase
     driver = GraphDatabase.driver(URI, auth=(USER, PASS))
     with driver.session() as session:
         session.run("MERGE (d:Document {id: $id, title: $title})", id=document_id, title=result.document.title)
         # Create node for each table and link
         for t in result.document.tables:
             session.run("""
                 MATCH (d:Document {id: $docid})
                 CREATE (t:Table {id: $tableid, caption: $caption})-[:PART_OF]->(d)
             """, docid=document_id, tableid=f"{document_id}_{t.index}", caption=t.caption or f"Table {t.index}")
     ```
     This creates a Document node and a Table node for each table, with a relationship `PART_OF`. Further relationships could be created if we extract entities from the table (for example, link a Company mentioned in the table to a Company node in the graph).

6. **Post-processing & Notification** – After storage, the pipeline can send a notification or event indicating success. For example, produce a message to a Kafka "processed" topic or send a log to an audit service. If there is a downstream system (maybe an analytics dashboard or an AI model that uses the data), it can be triggered here. 

Each of these steps would be defined in the pipeline configuration. With Kubeflow, you’d write a Python pipeline function decorating each step with `@dsl.component` (or use YAML components). With Tekton, you’d have tasks and possibly use Tekton’s Catalog tasks for some common operations (like an S3 upload task).

**Sample Code Snippets:**

- *Using Docling to parse and extract tables:* 

  ```python
  from docling.document_converter import DocumentConverter
  converter = DocumentConverter()
  result = converter.convert("sample.pdf")
  # Export entire document to markdown (tables will be in Markdown format in it)
  md_content = result.document.export_to_markdown()
  print(md_content[:200])  # print first 200 chars of markdown for preview
  # Iterate tables
  for idx, table in enumerate(result.document.tables):
      df = table.export_to_dataframe()
      df.to_csv(f"doc_{result.document.metadata.id}_table_{idx}.csv", index=False)
  ```
  This snippet would output CSV files for each table in *sample.pdf*. The `export_to_markdown()` will show a markdown table representation as a quick way to visualize that it worked (each table in JSON becomes a Markdown table in text).

- *Using Camelot as an alternative (for PDF with tabular data):* 

  ```python
  import camelot
  tables = camelot.read_pdf("sample.pdf", flavor="stream")  # or "lattice" if ruled lines
  print(f"Detected {tables.n} tables")
  for i, table in enumerate(tables):
      print(table.parsing_report)  # show accuracy metrics
      table.to_json(f"sample_table_{i}.json")
  ```
  
  Camelot’s parsing report gives a quick quality measure ([Camelot: PDF Table Extraction for Humans — Camelot 1.0.0 documentation](https://camelot-py.readthedocs.io/#:~:text=%3E%3E%3E%20tables%5B0%5D%20,order%27%3A%201%2C%20%27page%27%3A%201)), and you can choose the flavor depending on if the PDF has visible lines or not. We might include both modes in a try: first try lattice, if it finds nothing, try stream mode.

- *Tesseract OCR usage example (for a standalone image table):*

  ```python
  try:
      from PIL import Image
      import pytesseract
  except ImportError:
      pass
  img = Image.open("table_image.png")
  text = pytesseract.image_to_string(img, config="--psm 6")  # PSM 6 assumes a uniform block of text
  print(text)
  ```
  This would output the text from the image. However, without layout guidance, this might be a jumbled result for a table. A better approach is using pytesseract’s ability to get bounding boxes (`image_to_data`) and then grouping text by coordinates into a table structure.

**Integration with OpenShift Components:** To tie this example to OpenShift: the container images for these steps would include the necessary libraries (e.g., a Docling image with its dependencies, an image with Camelot and OpenCV if needed, etc.). OpenShift Pipelines (Tekton) would run each in a pod. Data passing between steps can be done via persistent volume (e.g., the JSON saved to a shared volume that the next step reads) or via pipeline artifacts if supported. OpenShift Data Foundation can provide a persistent volume claim that all pipeline tasks share for intermediate files. For scaling, multiple pipeline runs can use separate PVCs or unique paths to avoid collision.

**Tools and Libraries:** We have already used Docling, Camelot, pytesseract in examples. Other notable libraries:
  - **PDFMiner/PDFPlumber**: for fine-grained PDF text extraction if needed (e.g., to manually implement a custom table detector, one could use PDFPlumber to get all text with coordinates and then cluster by Y position).
  - **OpenCV**: could be used to detect lines in scanned documents (line detection to assist table structure detection).
  - **PyMuPDF (fitz)**: a fast PDF parser that can render pages to images for OCR and extract text with positioning; can be a backup for certain PDFs that other libraries can’t handle.
  - **NLTK/spaCy**: for any NLP on the text, such as entity recognition (which might feed the graph DB).
  - **LangChain or LlamaIndex**: these can be integrated if we go into retrieval augmentation, but for pre-processing they’re not needed except Docling already connects to them for RAG use cases ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=into%20formats%20like%20Markdown%20or,answering%20tasks)).

Finally, an example configuration for Tekton (pseudo-YAML):
```yaml
apiVersion: tekton.dev/v1beta1
kind: Pipeline
metadata: 
  name: doc-ingest-pipeline
spec:
  params:
    - name: document-path
      type: string
  tasks:
    - name: parse-doc
      image: myregistry/docling:latest
      script: |
        python -c "
        from docling.document_converter import DocumentConverter
        dc = DocumentConverter(); res = dc.convert('${params.document-path}');
        open('/workspace/output/doc.json','w').write(res.document.export_to_json())"
      results:
        - name: doc-json-path
          value: "/workspace/output/doc.json"
    - name: extract-tables
      runAfter: [parse-doc]
      image: myregistry/python:3.10
      script: |
        python -c "
        import json, pandas as pd;
        doc = json.load(open('${results.parse-doc.doc-json-path}'));
        tables = doc.get('tables', []); 
        for i, table in enumerate(tables):
            df = pd.DataFrame(table['rows']);
            df.to_csv(f'/workspace/output/table_{i}.csv', index=False)"
    - name: index-elastic
      runAfter: [extract-tables]
      image: myregistry/elastic-tools:latest 
      script: |
        # (Pseudo code) use curl or elasticsearch client to index the doc.json and tables
```
This is a rough illustration: the first task runs Docling in a Python image, second uses pandas to convert to CSV, third indexes to Elastic. In a real scenario, one might use more robust error handling and separation (and possibly not convert to CSV if JSON is sufficient). 

**Compliance and Configurations:** We would also include configuration for things like pipeline security (Tekton’s service account with limited permissions), resource limits for each task (e.g., give the parse-doc step a memory limit and request based on expected document sizes), and node selectors if we want certain tasks on GPU nodes. For example, `parse-doc` task could have a nodeSelector for `GPU=true` if Docling can use a GPU.

This example pipeline demonstrates a full path from ingestion to storage. It is modular, so we can plug in different extraction tools as needed (e.g., replace Camelot with Tabula Java if some PDFs parse better there, or add a step that uses Granite LLM for an extra analysis). The flexibility of OpenShift and the variety of tools ensure we can adapt the pipeline to multiple domains: for healthcare, we might add a de-identification step; for law enforcement, we might integrate with a secure vault or an entity resolution system; for financial services, we might directly connect the output to analytics platforms or BI tools. Each use case can be addressed by slight variations in the pipeline, but the core – **a scalable, AI-powered document pre-processing pipeline that preserves structured table data** – remains consistent across all. 

**References:** This solution design draws upon best practices and tools from recent advancements in document AI. IBM’s open-source Docling project ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=Docling%20is%20an%20upstream%20open,answering%20tasks)), with its use of vision models and TableFormer ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=The%20second%20model%2C%20TableFormer%2C%20is,recognition%20tools)), provides a state-of-the-art backbone for parsing. The importance of maintaining context (multi-page tables, etc.) is highlighted by Red Hat’s AI workflows ([Docling: The missing document processing companion for generative AI](https://www.redhat.com/en/blog/docling-missing-document-processing-companion-generative-ai#:~:text=This%20is%20not%20about%20a,where%20it%20is%20being%20extracted)). Techniques for multimodal extraction are informed by industry blueprints (e.g., NVIDIA’s pipeline for PDF extraction which separates text and tables for specialized processing ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=%2A%20nv,reading%20order%20of%20the%20table)) ([Build an Enterprise-Scale Multimodal PDF Data Extraction Pipeline with an NVIDIA AI Blueprint | NVIDIA Technical Blog](https://developer.nvidia.com/blog/build-an-enterprise-scale-multimodal-document-retrieval-pipeline-with-nvidia-nim-agent-blueprint/#:~:text=,reading%20order%20of%20the%20table))). The decision to combine vector search and graph databases is guided by understanding their complementary strengths in analytics ([Vector database vs. graph database: Knowledge Graph impact - Writer](https://writer.com/engineering/vector-database-vs-graph-database/#:~:text=searches%20but%20lack%20the%20context,for%20use%20in%20business%20settings)) ([Vector database vs. graph database: Understanding the differences | Elastic Blog](https://www.elastic.co/blog/vector-database-vs-graph-database#:~:text=What%20are%20graph%20databases%3F)). By leveraging OpenShift’s containerized pipeline orchestration and IBM’s AI technology, this design provides a robust, scalable approach to document ingestion that is adaptable to the stringent requirements of various industries and their compliance standards.  ([IBM is open-sourcing a new toolkit for document conversion - IBM Research](https://research.ibm.com/blog/docling-generative-AI#:~:text=It%20was%20also%20used%20to,forthcoming%20IBM%20Granite%20multimodal%20model))